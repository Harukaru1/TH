{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Text Processing\n",
    "\n",
    "1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ThisPc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ThisPc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ThisPc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ThisPc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"coldplay.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120 entries, 0 to 119\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Artist  120 non-null    object\n",
      " 1   Song    120 non-null    object\n",
      " 2   Link    120 non-null    object\n",
      " 3   Lyrics  120 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I turn the music up, I got my records on  \n",
      "I shut the world outside until the lights come on  \n",
      "Maybe the streets alight, maybe the trees are gone  \n",
      "I feel my heart start beating to my favourite song  \n",
      "  \n",
      "And all the kids they dance, all the kids all night  \n",
      "Until Monday morning feels another life  \n",
      "I turn the music up  \n",
      "I'm on a roll this time  \n",
      "And heaven is in sight  \n",
      "  \n",
      "I turn the music up, I got my records on  \n",
      "From underneath the rubble sing a rebel song  \n",
      "Don't want to see another generation drop  \n",
      "I'd rather be a comma than a full stop  \n",
      "  \n",
      "Maybe I'm in the black, maybe I'm on my knees  \n",
      "Maybe I'm in the gap between the two trapezes  \n",
      "But my heart is beating and my pulses start  \n",
      "Cathedrals in my heart  \n",
      "  \n",
      "As we saw oh this light I swear you, emerge blinking into  \n",
      "To tell me it's alright  \n",
      "As we soar walls, every siren is a symphony  \n",
      "And every tear's a waterfall  \n",
      "Is a waterfall  \n",
      "Oh  \n",
      "Is a waterfall  \n",
      "Oh oh oh  \n",
      "Is a is a waterfall  \n",
      "Every tear  \n",
      "Is a waterfall  \n",
      "Oh oh oh  \n",
      "  \n",
      "So you can hurt, hurt me bad  \n",
      "But still I'll raise the flag  \n",
      "  \n",
      "Oh  \n",
      "It was a wa wa wa wa wa-aterfall  \n",
      "A wa wa wa wa wa-aterfall  \n",
      "  \n",
      "Every tear  \n",
      "Every tear  \n",
      "Every teardrop is a waterfall  \n",
      "  \n",
      "Every tear  \n",
      "Every tear  \n",
      "Every teardrop is a waterfall  \n",
      "  \n",
      "Every tear  \n",
      "Every tear  \n",
      "Every teardrop is a waterfall\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_title = 'Every Teardrop Is A Waterfall'\n",
    "lyrics = df.loc[df['Song'] == song_title, 'Lyrics'].values[0]\n",
    "print(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'turn', 'the', 'music', 'up', ',', 'I', 'got', 'my', 'records', 'on', 'I', 'shut', 'the', 'world', 'outside', 'until', 'the', 'lights', 'come', 'on', 'Maybe', 'the', 'streets', 'alight', ',', 'maybe', 'the', 'trees', 'are', 'gone', 'I', 'feel', 'my', 'heart', 'start', 'beating', 'to', 'my', 'favourite', 'song', 'And', 'all', 'the', 'kids', 'they', 'dance', ',', 'all', 'the', 'kids', 'all', 'night', 'Until', 'Monday', 'morning', 'feels', 'another', 'life', 'I', 'turn', 'the', 'music', 'up', 'I', \"'m\", 'on', 'a', 'roll', 'this', 'time', 'And', 'heaven', 'is', 'in', 'sight', 'I', 'turn', 'the', 'music', 'up', ',', 'I', 'got', 'my', 'records', 'on', 'From', 'underneath', 'the', 'rubble', 'sing', 'a', 'rebel', 'song', 'Do', \"n't\", 'want', 'to', 'see', 'another', 'generation', 'drop', 'I', \"'d\", 'rather', 'be', 'a', 'comma', 'than', 'a', 'full', 'stop', 'Maybe', 'I', \"'m\", 'in', 'the', 'black', ',', 'maybe', 'I', \"'m\", 'on', 'my', 'knees', 'Maybe', 'I', \"'m\", 'in', 'the', 'gap', 'between', 'the', 'two', 'trapezes', 'But', 'my', 'heart', 'is', 'beating', 'and', 'my', 'pulses', 'start', 'Cathedrals', 'in', 'my', 'heart', 'As', 'we', 'saw', 'oh', 'this', 'light', 'I', 'swear', 'you', ',', 'emerge', 'blinking', 'into', 'To', 'tell', 'me', 'it', \"'s\", 'alright', 'As', 'we', 'soar', 'walls', ',', 'every', 'siren', 'is', 'a', 'symphony', 'And', 'every', 'tear', \"'s\", 'a', 'waterfall', 'Is', 'a', 'waterfall', 'Oh', 'Is', 'a', 'waterfall', 'Oh', 'oh', 'oh', 'Is', 'a', 'is', 'a', 'waterfall', 'Every', 'tear', 'Is', 'a', 'waterfall', 'Oh', 'oh', 'oh', 'So', 'you', 'can', 'hurt', ',', 'hurt', 'me', 'bad', 'But', 'still', 'I', \"'ll\", 'raise', 'the', 'flag', 'Oh', 'It', 'was', 'a', 'wa', 'wa', 'wa', 'wa', 'wa-aterfall', 'A', 'wa', 'wa', 'wa', 'wa', 'wa-aterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'is', 'a', 'waterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'is', 'a', 'waterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'is', 'a', 'waterfall']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "from nltk import word_tokenize\n",
    "words = word_tokenize(lyrics)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'turn', 'the', 'music', 'up', 'I', 'got', 'my', 'records', 'on', 'I', 'shut', 'the', 'world', 'outside', 'until', 'the', 'lights', 'come', 'on', 'Maybe', 'the', 'streets', 'alight', 'maybe', 'the', 'trees', 'are', 'gone', 'I', 'feel', 'my', 'heart', 'start', 'beating', 'to', 'my', 'favourite', 'song', 'And', 'all', 'the', 'kids', 'they', 'dance', 'all', 'the', 'kids', 'all', 'night', 'Until', 'Monday', 'morning', 'feels', 'another', 'life', 'I', 'turn', 'the', 'music', 'up', 'I', \"'m\", 'on', 'a', 'roll', 'this', 'time', 'And', 'heaven', 'is', 'in', 'sight', 'I', 'turn', 'the', 'music', 'up', 'I', 'got', 'my', 'records', 'on', 'From', 'underneath', 'the', 'rubble', 'sing', 'a', 'rebel', 'song', 'Do', \"n't\", 'want', 'to', 'see', 'another', 'generation', 'drop', 'I', \"'d\", 'rather', 'be', 'a', 'comma', 'than', 'a', 'full', 'stop', 'Maybe', 'I', \"'m\", 'in', 'the', 'black', 'maybe', 'I', \"'m\", 'on', 'my', 'knees', 'Maybe', 'I', \"'m\", 'in', 'the', 'gap', 'between', 'the', 'two', 'trapezes', 'But', 'my', 'heart', 'is', 'beating', 'and', 'my', 'pulses', 'start', 'Cathedrals', 'in', 'my', 'heart', 'As', 'we', 'saw', 'oh', 'this', 'light', 'I', 'swear', 'you', 'emerge', 'blinking', 'into', 'To', 'tell', 'me', 'it', \"'s\", 'alright', 'As', 'we', 'soar', 'walls', 'every', 'siren', 'is', 'a', 'symphony', 'And', 'every', 'tear', \"'s\", 'a', 'waterfall', 'Is', 'a', 'waterfall', 'Oh', 'Is', 'a', 'waterfall', 'Oh', 'oh', 'oh', 'Is', 'a', 'is', 'a', 'waterfall', 'Every', 'tear', 'Is', 'a', 'waterfall', 'Oh', 'oh', 'oh', 'So', 'you', 'can', 'hurt', 'hurt', 'me', 'bad', 'But', 'still', 'I', \"'ll\", 'raise', 'the', 'flag', 'Oh', 'It', 'was', 'a', 'wa', 'wa', 'wa', 'wa', 'wa-aterfall', 'A', 'wa', 'wa', 'wa', 'wa', 'wa-aterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'is', 'a', 'waterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'is', 'a', 'waterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'is', 'a', 'waterfall']\n"
     ]
    }
   ],
   "source": [
    "tokens_without_punctuation = [token for token in words if token not in string.punctuation]\n",
    "\n",
    "print(tokens_without_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['turn', 'music', ',', 'got', 'records', 'shut', 'world', 'outside', 'lights', 'come', 'Maybe', 'streets', 'alight', ',', 'maybe', 'trees', 'gone', 'feel', 'heart', 'start', 'beating', 'favourite', 'song', 'kids', 'dance', ',', 'kids', 'night', 'Monday', 'morning', 'feels', 'another', 'life', 'turn', 'music', \"'m\", 'roll', 'time', 'heaven', 'sight', 'turn', 'music', ',', 'got', 'records', 'underneath', 'rubble', 'sing', 'rebel', 'song', \"n't\", 'want', 'see', 'another', 'generation', 'drop', \"'d\", 'rather', 'comma', 'full', 'stop', 'Maybe', \"'m\", 'black', ',', 'maybe', \"'m\", 'knees', 'Maybe', \"'m\", 'gap', 'two', 'trapezes', 'heart', 'beating', 'pulses', 'start', 'Cathedrals', 'heart', 'saw', 'oh', 'light', 'swear', ',', 'emerge', 'blinking', 'tell', \"'s\", 'alright', 'soar', 'walls', ',', 'every', 'siren', 'symphony', 'every', 'tear', \"'s\", 'waterfall', 'waterfall', 'Oh', 'waterfall', 'Oh', 'oh', 'oh', 'waterfall', 'Every', 'tear', 'waterfall', 'Oh', 'oh', 'oh', 'hurt', ',', 'hurt', 'bad', 'still', \"'ll\", 'raise', 'flag', 'Oh', 'wa', 'wa', 'wa', 'wa', 'wa-aterfall', 'wa', 'wa', 'wa', 'wa', 'wa-aterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lyrics = df.loc[df['Song'] == song_title, 'Lyrics'].values[0]\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "word_tokens = word_tokenize(lyrics)\n",
    "\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "\n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w.lower() not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['turn', 'music', ',', 'got', 'record', 'shut', 'world', 'outside', 'light', 'come', 'Maybe', 'street', 'alight', ',', 'maybe', 'tree', 'gone', 'feel', 'heart', 'start', 'beating', 'favourite', 'song', 'kid', 'dance', ',', 'kid', 'night', 'Monday', 'morning', 'feel', 'another', 'life', 'turn', 'music', \"'m\", 'roll', 'time', 'heaven', 'sight', 'turn', 'music', ',', 'got', 'record', 'underneath', 'rubble', 'sing', 'rebel', 'song', \"n't\", 'want', 'see', 'another', 'generation', 'drop', \"'d\", 'rather', 'comma', 'full', 'stop', 'Maybe', \"'m\", 'black', ',', 'maybe', \"'m\", 'knee', 'Maybe', \"'m\", 'gap', 'two', 'trapeze', 'heart', 'beating', 'pulse', 'start', 'Cathedrals', 'heart', 'saw', 'oh', 'light', 'swear', ',', 'emerge', 'blinking', 'tell', \"'s\", 'alright', 'soar', 'wall', ',', 'every', 'siren', 'symphony', 'every', 'tear', \"'s\", 'waterfall', 'waterfall', 'Oh', 'waterfall', 'Oh', 'oh', 'oh', 'waterfall', 'Every', 'tear', 'waterfall', 'Oh', 'oh', 'oh', 'hurt', ',', 'hurt', 'bad', 'still', \"'ll\", 'raise', 'flag', 'Oh', 'wa', 'wa', 'wa', 'wa', 'wa-aterfall', 'wa', 'wa', 'wa', 'wa', 'wa-aterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_sentence]\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('turn', 'NN'), ('music', 'NN'), (',', ','), ('got', 'VBD'), ('record', 'NN'), ('shut', 'NN'), ('world', 'NN'), ('outside', 'IN'), ('light', 'JJ'), ('come', 'VBP'), ('Maybe', 'NNP'), ('street', 'NN'), ('alight', 'NN'), (',', ','), ('maybe', 'RB'), ('tree', 'VBP'), ('gone', 'VBN'), ('feel', 'JJ'), ('heart', 'NN'), ('start', 'NN'), ('beating', 'VBG'), ('favourite', 'NN'), ('song', 'NN'), ('kid', 'NN'), ('dance', 'NN'), (',', ','), ('kid', 'VB'), ('night', 'NN'), ('Monday', 'NNP'), ('morning', 'NN'), ('feel', 'NN'), ('another', 'DT'), ('life', 'NN'), ('turn', 'NN'), ('music', 'NN'), (\"'m\", 'VBP'), ('roll', 'JJ'), ('time', 'NN'), ('heaven', 'VBN'), ('sight', 'JJ'), ('turn', 'NN'), ('music', 'NN'), (',', ','), ('got', 'VBD'), ('record', 'JJ'), ('underneath', 'NN'), ('rubble', 'JJ'), ('sing', 'VBG'), ('rebel', 'NN'), ('song', 'VBP'), (\"n't\", 'RB'), ('want', 'VB'), ('see', 'VB'), ('another', 'DT'), ('generation', 'NN'), ('drop', 'NN'), (\"'d\", 'MD'), ('rather', 'RB'), ('comma', 'VB'), ('full', 'JJ'), ('stop', 'NN'), ('Maybe', 'NNP'), (\"'m\", 'VBP'), ('black', 'JJ'), (',', ','), ('maybe', 'RB'), (\"'m\", 'VBP'), ('knee', 'JJ'), ('Maybe', 'NNP'), (\"'m\", 'VBP'), ('gap', 'JJ'), ('two', 'CD'), ('trapeze', 'JJ'), ('heart', 'NN'), ('beating', 'VBG'), ('pulse', 'JJ'), ('start', 'NN'), ('Cathedrals', 'NNP'), ('heart', 'NN'), ('saw', 'VBD'), ('oh', 'JJ'), ('light', 'JJ'), ('swear', 'NN'), (',', ','), ('emerge', 'VB'), ('blinking', 'VBG'), ('tell', 'NN'), (\"'s\", 'POS'), ('alright', 'NN'), ('soar', 'VB'), ('wall', 'NN'), (',', ','), ('every', 'DT'), ('siren', 'NN'), ('symphony', 'NN'), ('every', 'DT'), ('tear', 'NN'), (\"'s\", 'POS'), ('waterfall', 'NN'), ('waterfall', 'NN'), ('Oh', 'NNP'), ('waterfall', 'NN'), ('Oh', 'NNP'), ('oh', 'VBZ'), ('oh', 'UH'), ('waterfall', 'NN'), ('Every', 'NNP'), ('tear', 'NN'), ('waterfall', 'NN'), ('Oh', 'NNP'), ('oh', 'VBZ'), ('oh', 'JJ'), ('hurt', 'NN'), (',', ','), ('hurt', 'NN'), ('bad', 'JJ'), ('still', 'RB'), (\"'ll\", 'MD'), ('raise', 'VB'), ('flag', 'NN'), ('Oh', 'NNP'), ('wa', 'NN'), ('wa', 'NN'), ('wa', 'NN'), ('wa', 'VBD'), ('wa-aterfall', 'JJ'), ('wa', 'NN'), ('wa', 'NN'), ('wa', 'NN'), ('wa', 'VBD'), ('wa-aterfall', 'JJ'), ('Every', 'NNP'), ('tear', 'NN'), ('Every', 'NNP'), ('tear', 'NN'), ('Every', 'NNP'), ('teardrop', 'NN'), ('waterfall', 'NN'), ('Every', 'NNP'), ('tear', 'NN'), ('Every', 'NNP'), ('tear', 'NN'), ('Every', 'NNP'), ('teardrop', 'NN'), ('waterfall', 'NN'), ('Every', 'NNP'), ('tear', 'NN'), ('Every', 'NNP'), ('tear', 'NN'), ('Every', 'NNP'), ('teardrop', 'NN'), ('waterfall', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "pos_tags = nltk.pos_tag(lemmatized_tokens)\n",
    "\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    output = np.asarray(pos_tag)\n",
    "    for i in range(len(pos_tag)):\n",
    "        if pos_tag[i][1].startswith('J'):\n",
    "            output[i][1] = wordnet.ADJ\n",
    "        elif pos_tag[i][1].startswith('V'):\n",
    "            output[i][1] = wordnet.VERB\n",
    "        elif pos_tag[i][1].startswith('R'):\n",
    "            output[i][1] = wordnet.ADV\n",
    "        else:\n",
    "            output[i][1] = wordnet.NOUN\n",
    "    return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"coldplay.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120 entries, 0 to 119\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Artist  120 non-null    object\n",
      " 1   Song    120 non-null    object\n",
      " 2   Link    120 non-null    object\n",
      " 3   Lyrics  120 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 1776)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "bow = vectorizer.fit_transform(df['Lyrics'])\n",
    "\n",
    "print( bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>2000</th>\n",
       "      <th>2gether</th>\n",
       "      <th>76543</th>\n",
       "      <th>aaaaaah</th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaah</th>\n",
       "      <th>about</th>\n",
       "      <th>above</th>\n",
       "      <th>achin</th>\n",
       "      <th>...</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yuletide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows Ã— 1776 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     10  2000  2gether  76543  aaaaaah  aaaaah  aaaah  about  above  achin  \\\n",
       "0     0     0        0      0        0       0      0      0      0      0   \n",
       "1     0     0        0      0        0       0      0      0      0      0   \n",
       "2     0     0        0      0        0       0      0      0      0      0   \n",
       "3     0     0        0      0        0       0      0      0      0      0   \n",
       "4     0     0        0      0        0       0      0      0      0      0   \n",
       "..   ..   ...      ...    ...      ...     ...    ...    ...    ...    ...   \n",
       "115   0     0        0      0        0       0      0      1      2      0   \n",
       "116   0     0        0      0        0       0      0      0      0      0   \n",
       "117   0     0        1      0        0       0      0      0      0      0   \n",
       "118   0     0        0      0        0       0      0      0      0      0   \n",
       "119   0     0        0      0        0       0      0      0      0      0   \n",
       "\n",
       "     ...  yellow  yes  yesterday  yet  you  young  your  yours  yourself  \\\n",
       "0    ...       0    0          0    0    4      0     4      0         2   \n",
       "1    ...       0    0          0    0    0      0     0      0         0   \n",
       "2    ...       0    0          0    0    0      0     0      0         0   \n",
       "3    ...       0    0          0    0   16      0     0      0         0   \n",
       "4    ...       0    0          0    0    2      0     0      0         0   \n",
       "..   ...     ...  ...        ...  ...  ...    ...   ...    ...       ...   \n",
       "115  ...       0    0          0    0    5      0     3      0         0   \n",
       "116  ...       0    0          0    0    9      0     0      0         0   \n",
       "117  ...       0    0          0    0    7      0     4      0         0   \n",
       "118  ...       0    0          0    0   16      0     1      0         0   \n",
       "119  ...       0    0          0    0    5      0     0      0         0   \n",
       "\n",
       "     yuletide  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "..        ...  \n",
       "115         0  \n",
       "116         0  \n",
       "117         0  \n",
       "118         0  \n",
       "119         0  \n",
       "\n",
       "[120 rows x 1776 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "bow_df = pd.DataFrame(bow.toarray(), columns =feature_names)\n",
    "\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_bow = bow_df.sum()\n",
    "sum_bow.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you    994\n",
      "the    777\n",
      "and    650\n",
      "to     481\n",
      "it     458\n",
      "oh     334\n",
      "in     318\n",
      "me     314\n",
      "my     288\n",
      "on     285\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "word_counts = bow_df.sum()\n",
    "top_10 = word_counts.nlargest(10)\n",
    "\n",
    "print(top_10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
